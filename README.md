```md

\# StoryGen ‚Äî P1RV Sujet 9 (G√©n√©ration d'histoires C++ / SLM)



\## üéØ Objectif

Ce projet propose une \*\*CLI C++\*\* (`storygen.exe`) qui lance un \*\*Small Language Model (SLM)\*\* en local via \*\*llama.cpp\*\* afin de g√©n√©rer une histoire √† partir d‚Äôun prompt (fichier texte), avec param√®tres de g√©n√©ration configurables.



\- Plateforme : \*\*Windows\*\*

\- Langage : \*\*C++\*\*

\- Backend SLM : \*\*llama.cpp\*\* (`llama-cli.exe`)

\- Mod√®les : \*\*GGUF\*\* (ex: Mistral 7B Instruct Q4)



---



\## üìÅ Structure du d√©p√¥t

```



P1RV/

‚îú‚îÄ app/                  # code C++ (main.cpp)

‚îú‚îÄ CMakeLists.txt        # build de storygen

‚îú‚îÄ llama.cpp/            # submodule llama.cpp

‚îú‚îÄ models/               # mod√®les GGUF (non inclus dans le zip)

‚îú‚îÄ prompt.txt            # prompt d'entr√©e (exemple)

‚îî‚îÄ out/                  # r√©pertoires de build (g√©n√©r√©s)



````



---



\## ‚úÖ Pr√©requis (Windows)

\- \*\*Visual Studio 2022\*\* (Desktop development with C++)

\- \*\*CMake\*\* (via VS ou install√© s√©par√©ment)

\- \*\*Ninja\*\* (recommand√©, via VS possible)

\- Git (si tu r√©cup√®res le d√©p√¥t avec submodule)



---



\## üîÅ R√©cup√©ration du submodule llama.cpp (si n√©cessaire)

Si `llama.cpp/` est un submodule :



```bat

git submodule update --init --recursive

````



---



\## üì¶ Mod√®le GGUF (√† placer dans `models/`)



Le mod√®le \*\*n‚Äôest pas inclus\*\* dans le zip (taille trop importante).



1\. T√©l√©charger un mod√®le GGUF (exemple utilis√© durant le projet) :



\* `mistral-7b-instruct-v0.2.Q4\_0.gguf`



2\. Le placer ici :



```

P1RV\\models\\mistral-7b-instruct-v0.2.Q4\_0.gguf

```



> Remarque : la CLI accepte aussi `--model <path>` si tu veux utiliser un autre GGUF.



---



\## üõ†Ô∏è Build de llama.cpp (pour obtenir llama-cli.exe)



Depuis la racine `P1RV` :



```bat

cmake -S llama.cpp -B llama.cpp\\build -G Ninja -DCMAKE\_BUILD\_TYPE=Release

cmake --build llama.cpp\\build

```



Apr√®s compilation, tu dois avoir (selon config) :



```

P1RV\\llama.cpp\\build\\bin\\Release\\llama-cli.exe

```



---



\## üß± Build de storygen (CLI C++)



Depuis la racine `P1RV` :



\### Debug



```bat

cmake -S . -B out\\build\\x64-Debug -G Ninja -DCMAKE\_BUILD\_TYPE=Debug

cmake --build out\\build\\x64-Debug

```



\### Release



```bat

cmake -S . -B out\\build\\x64-Release -G Ninja -DCMAKE\_BUILD\_TYPE=Release

cmake --build out\\build\\x64-Release

```



---



\## ‚ñ∂Ô∏è Ex√©cution (d√©mo live)



\### D√©mo simple + logs



```bat

out\\build\\x64-Release\\storygen.exe --verbose

```



\### Param√®tres (exemples)



```bat

out\\build\\x64-Release\\storygen.exe -n 300 -t 0.9 -p 0.95 -s 123 --verbose

```



\### √âcrire la sortie dans un fichier



```bat

out\\build\\x64-Release\\storygen.exe --out out.txt

```



---



\## üß∞ Options CLI



\* `-h, --help` : affiche l‚Äôaide

\* `-m, --model <path>` : chemin du mod√®le GGUF

\* `-f, --prompt <path>` : chemin du fichier prompt

\* `-n, --n-predict <int>` : nombre de tokens √† g√©n√©rer

\* `-s, --seed <int>` : seed (reproductibilit√©)

\* `-t, --temp <float>` : temp√©rature (cr√©ativit√©)

\* `-p, --top-p <float>` : top-p (noyau de probas)

\* `--verbose` : affiche les chemins r√©solus + commande ex√©cut√©e

\* `--out <path>` : √©crit stdout/stderr dans un fichier

\* `--single-turn` : (par d√©faut) 1 prompt ‚Üí 1 r√©ponse ‚Üí exit



---



\## üß™ Prompt d‚Äôexemple



Le fichier `prompt.txt` contient un prompt simple.

Exemple attendu (r√©sum√©) :



\* `Loading model...`

\* g√©n√©ration d‚Äôune histoire

\* `Exiting...` (fin propre gr√¢ce au mode single-turn)



---



\## üß® D√©pannage rapide



\### 1) ‚Äúllama-cli introuvable‚Äù



V√©rifie que le binaire existe :



```

P1RV\\llama.cpp\\build\\bin\\Release\\llama-cli.exe

```



Sinon recompile llama.cpp en Release.



\### 2) ‚Äúmod√®le introuvable‚Äù



V√©rifie la pr√©sence du `.gguf` dans `models/`, ou lance avec :



```bat

storygen.exe --model "C:\\...\\monmodele.gguf"

```



\### 3) Le programme ne se termine pas



Assure-toi d‚Äô√™tre en mode \*\*single-turn\*\* (par d√©faut).

Si tu as modifi√© le code, repasse en `--single-turn`.



\### 4) Probl√®mes x86/x64 (link / libs)



Utiliser un terminal ‚ÄúDeveloper PowerShell for VS 2022‚Äù et v√©rifier que la toolchain cible est bien \*\*x64\*\*.



---



\## üßæ Soutenance (infos)



\* Projet : \*\*P1RV ‚Äì Sujet 9 : G√©n√©ration d‚Äôhistoires C++ / SLM\*\*

\* Travail en autonomie (1 √©tudiant)

\* Temps estim√© : \*\*~15h\*\*



```



---



Si tu veux, je peux aussi te g√©n√©rer un \*\*ZIP ‚Äúrendu‚Äù\*\* recommand√© (structure + fichiers √† inclure / exclure), et/ou t‚Äô√©crire une version \*\*README courte\*\* (1 page max) sp√©ciale d√©p√¥t Hippocampus.

::contentReference\[oaicite:0]{index=0}

```



